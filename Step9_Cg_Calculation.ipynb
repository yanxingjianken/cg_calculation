{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98be0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions_cg import *\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from netCDF4 import Dataset\n",
    "import scipy as scp\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "from cartopy import crs\n",
    "import cartopy\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "from scipy.signal import tukey\n",
    "import re\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dca9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ERA5'\n",
    "path_data = '/scratch/negishi/yan481/ERA5/'\n",
    "path_ncr = '/apps/spack/negishi/apps/nco/4.6.7-gcc-4.8.5-gsgvpvo/bin/ncrcat'\n",
    "\n",
    "\n",
    "path_outputs = f'/scratch/negishi/yan481/{name}/'\n",
    "path_figures = f'/home/yan481/Cg/{name}/'\n",
    "seasons = True\n",
    "variable = 'v300'\n",
    "nc_name = f'{variable}_{name}_6h_1979_1999.nc'\n",
    "L0 = 20 #Longitude\n",
    "E0 = 15 #[m/s] Threshold \n",
    "lambda1 = 2000  # km\n",
    "lambda2 = 10000  # km\n",
    "lambdaE = 4000 # km\n",
    "\n",
    "# ncfile = Dataset(f'{path_outputs}{nc_name}')\n",
    "ncfile = Dataset(f'f'/scratch/bell/castanev/{'ERA5'}/f'{'v300'}_{'ERA5'}_6h_1979_1999.nc')\n",
    "lats = np.array(ncfile['lat'])\n",
    "pos_NH = np.where(lats>=20)[0]\n",
    "lats_NH = lats[lats>=20]\n",
    "lons = np.array(ncfile['lon'])\n",
    "time = np.array(ncfile['time'])[:]\n",
    "dates_d = [dt.datetime.strptime(str(int(i)), \"%Y%m%d%H\") for i in time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if seasons == True:\n",
    "    df = pd.DataFrame(index=dates_d, data=np.reshape(np.array(ncfile[variable])[:,pos_NH,:], [len(dates_d), lats_NH.shape[0] * lons.shape[0]])) \n",
    "\n",
    "    def calculate_day_climatology(tuple, time_col='time', dayofyear_col='dayofyear'): \n",
    "        n = tuple[0]; days = tuple[1]\n",
    "        day_df1 = df[df[dayofyear_col].isin(days)]\n",
    "        day_climatology = day_df1.groupby([dayofyear_col, time_col]).transform('mean')\n",
    "        day_climatology.index = day_df1.index\n",
    "        # anoma = day_df1 - day_climatology\n",
    "        for i in np.unique(day_climatology.index.year):\n",
    "            day_climatology_i = day_climatology[day_climatology.index.year == i]\n",
    "            save_nc_3d(f'{path_outputs}temporal/clima_test/{i}{n}.nc', np.round(np.array(day_climatology_i),3).reshape(day_climatology_i.shape[0], lats_NH.shape[0], lons.shape[0]), lats_NH, lons, np.array([ii.strftime(\"%Y%m%d%H\") for ii in day_climatology_i.index]), 'v300')\n",
    "\n",
    "\n",
    "    unique_days = np.arange(1,367,1)\n",
    "    intervals = [sublist.tolist() for sublist in np.array_split(unique_days, 15)]\n",
    "    time_col='time'; df[time_col] = df.index.time\n",
    "    dayofyear_col='dayofyear'; df[dayofyear_col] = df.index.dayofyear\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        args_list = [(n, days) for n, days in enumerate(intervals)]\n",
    "        with multiprocessing.Pool(processes=150) as pool:\n",
    "            pool.map(calculate_day_climatology, args_list)\n",
    "\n",
    "\n",
    "    dates_d_index = pd.DatetimeIndex(dates_d)\n",
    "    def calculate_anoma(ncfile_i):\n",
    "        time_i = np.array(Dataset(f'{path_outputs}temporal/clima/clima{str(ncfile_i)}.nc')['time']); time_i = np.array([int(i) for i in time_i])\n",
    "        clima_i = np.array(Dataset(f'{path_outputs}temporal/clima/clima{str(ncfile_i)}.nc')[variable])  \n",
    "        for y in np.unique(dates_d_index[np.where(dates_d_index.dayofyear.isin(time_i))[0]].year):\n",
    "            pos = np.where((dates_d_index.year==y)&(dates_d_index.dayofyear.isin(time_i)))[0]\n",
    "            var_i = np.array(ncfile[variable])[pos,:,:]; var_i = var_i[:,pos_NH,:]\n",
    "            anoma_i = var_i - clima_i[:len(pos)]\n",
    "            save_nc_3d(f'{path_outputs}temporal/anoma/{str(y)}{str(ncfile_i)}.nc', anoma_i, lats_NH, lons, time[pos], 'v300_anom')\n",
    "\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/clima/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    for ncfile_i in list_data:\n",
    "        calculate_anoma(ncfile_i)\n",
    "\n",
    "\n",
    "    def filter_chunck(ncfile_i):\n",
    "        df_anom = np.array(Dataset(f'{path_outputs}temporal/anoma/{str(ncfile_i)}.nc')['v300_anom'])\n",
    "        time_i = np.array(Dataset(f'{path_outputs}temporal/anoma/{ncfile_i}.nc')['time'])\n",
    "        series_t_filter = np.zeros([len(time_i), len(lats_NH), len(lons)]) * np.nan\n",
    "        for t in range(len(time_i)):\n",
    "            for num, lat in enumerate(lats_NH):\n",
    "                serie = df_anom[t, num, :]\n",
    "                km_1degree = np.pi*2*6371*np.cos(np.radians(lat))/360 # [km]\n",
    "                k1 = 360*km_1degree/lambda1\n",
    "                k2 = 360*km_1degree/lambda2\n",
    "                series_t_filter[t,num,:] = fourier_filter_window(serie_i=serie, lons = lons, km_1degree = km_1degree, k1 = k1, k2 = k2)\n",
    "        save_nc_3d(f'{path_outputs}temporal/anoma_filter/{str(ncfile_i)}.nc', series_t_filter, lats_NH, lons, time_i, 'v300_anom')\n",
    "\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/anoma/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    for sublist_data in [sublist.tolist() for sublist in np.array_split(list_data, 60)]:\n",
    "        if __name__ == \"__main__\":\n",
    "            with multiprocessing.Pool(processes=120) as pool:\n",
    "                pool.map(filter_chunck, sublist_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_envelope(ncfile_i):\n",
    "        df_anom = np.array(Dataset(f'{path_outputs}temporal/anoma_filter/{str(ncfile_i)}.nc')['v300_anom'])\n",
    "        time_i = np.array(Dataset(f'{path_outputs}temporal/anoma_filter/{str(ncfile_i)}.nc')['time'])\n",
    "        E_thres = np.zeros_like(df_anom)*np.nan\n",
    "        E_prim = np.zeros_like(df_anom)*np.nan\n",
    "        for t in range(df_anom.shape[0]):\n",
    "            for num, lat in enumerate(lats_NH):\n",
    "                km_1degree = np.pi*2*6371*np.cos(np.radians(lat))/360 # [km]\n",
    "                serie = df_anom[t,num,:]\n",
    "                analytic_signal_filt = hilbert(serie)\n",
    "                Envelope = np.abs(analytic_signal_filt)\n",
    "                \n",
    "                freq  = np.fft.fftfreq(len(Envelope), 1) \n",
    "                wavelengths = (1/freq)*np.abs(lons[1]-lons[0])*km_1degree #[km]  \n",
    "                fourier = np.fft.fft(Envelope)/len(Envelope)\n",
    "                filter = np.where((abs(wavelengths) < 4500))[0] \n",
    "                fourier_filtered = np.copy(fourier)\n",
    "                fourier_filtered[filter] = 0\n",
    "                Envelope_filter = np.fft.ifft(fourier_filtered* len(serie))\n",
    "                \n",
    "                \n",
    "                # Cg ==========================================================================================\n",
    "                E_threshold = filter_threshold_lenght(Envelope_filter, lons, E0, L0)\n",
    "                E_thres[t,num,:] = E_threshold\n",
    "                if E_threshold.any() == np.nan: print(f'More than 1 event {np.array(dates_d)[t]}')\n",
    "                \n",
    "                events_data = {}\n",
    "                # Extract event data and store in the list\n",
    "                current_event = []\n",
    "                contt = 0\n",
    "                \n",
    "                for i, value in enumerate(E_threshold):\n",
    "                    if value: current_event.append(value)          \n",
    "                    else:\n",
    "                        if current_event:\n",
    "                            contt = contt+1\n",
    "                            events_data[contt] = pd.Series(current_event, index = np.arange(i-len(current_event)+1,i+1,1))\n",
    "                            current_event = []\n",
    "                    if ((i+1 == len(E_threshold)) & (i+1 == len(current_event))):\n",
    "                        contt = contt+1\n",
    "                        events_data[contt] = pd.Series(current_event, index = np.arange(i-len(current_event)+1,i+1,1))\n",
    "                        current_event = []\n",
    "                \n",
    "                for i in events_data.keys():\n",
    "                    media_object = events_data[i].mean()\n",
    "                    E_prim[t,num,events_data[i].index] = E_threshold[events_data[i].index] - media_object\n",
    "                \n",
    "                if (t in [400]) & (lat == 45.0):\n",
    "                    fig = plt.figure(figsize=[10, 6])\n",
    "                    ax = fig.add_subplot(1, 1, 1)\n",
    "                    plt.plot(df_anom[t, num, :], c='k')\n",
    "                    plt.plot(Envelope_filter, c='g')\n",
    "                    plt.plot(E_prim[t, num, :], c='orange')\n",
    "                    plt.savefig(path_figures + f'test{round(round(lat))}_{ncfile_i}_filtered_annual_clima6.png', dpi=500)\n",
    "\n",
    "        save_nc_3d(f'{path_outputs}temporal/E_thres/{str(ncfile_i)}.nc', E_thres, lats_NH, lons, time_i, 'E')\n",
    "        save_nc_3d(f'{path_outputs}temporal/E_prim/{str(ncfile_i)}.nc', E_prim, lats_NH, lons, time_i, 'E')\n",
    "\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/anoma_filter/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    for sublist_data in [sublist.tolist() for sublist in np.array_split(list_data, 60)]:\n",
    "        if __name__ == \"__main__\":\n",
    "            with multiprocessing.Pool(processes=120) as pool:\n",
    "                pool.map(calculate_envelope, sublist_data)\n",
    "\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/E_thres/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    nco_command = f\"module load nco && {path_ncr} {' '.join([f'{path_outputs}/temporal/E_thres/' + str(i) + '.nc' for i in list_data])} {os.path.join(path_outputs, 'temporal', f'E_thres_{name}_1979-1999.nc')}\"\n",
    "    result = subprocess.run(nco_command, shell=True)\n",
    "    print(result.returncode)\n",
    "\n",
    "\n",
    "    E_thres = np.array(Dataset(f'{path_outputs}temporal/E_thres_{name}_1979-1999.nc')['E'])\n",
    "    E_seasonal_mean = np.empty([4,len(lats_NH),len(lons)]) * np.nan\n",
    "    print(E_thres)\n",
    "    print(E_thres.shape)\n",
    "    time_i = np.array(Dataset(f'{path_outputs}temporal/E_thres_{name}_1979-1999.nc')['time'])\n",
    "    dates_d_i = [dt.datetime.strptime(str(int(i)), \"%Y%m%d%H\") for i in time_i]\n",
    "    for num, j, season in zip([0,1,2,3],[[12,1,2], [3,4,5], [6,7,8], [9,10,11]],['DJF','MAM','JJA','SON']):\n",
    "        pos_season = np.where((pd.to_datetime(dates_d_i).month == j[0]) ^ (pd.to_datetime(dates_d_i).month == j[1]) ^ (pd.to_datetime(dates_d_i).month == j[2]))[0]\n",
    "        E_seasonal_mean[num,:,:] = np.nanmean(E_thres[pos_season,:,:], axis=0)\n",
    "    save_nc_3d(f'{path_outputs}temporal/E_seasonal_mean.nc', E_seasonal_mean, lats_NH, lons, np.arange(0,4,1).astype(str), 'E')\n",
    "\n",
    "\n",
    "    E_seasonal_mean = np.array(Dataset(f'{path_outputs}temporal/E_seasonal_mean.nc')['E'])\n",
    "    for num, season in zip([0,1,2,3],['DJF','MAM','JJA','SON']):\n",
    "        vmin, vmax, colormap = colorm(10, 27, 15, 11, 'RdYlBu_r') \n",
    "        season_mean = E_seasonal_mean[num]\n",
    "        map_orhographic(lons, lats_NH, vmin, vmax, season_mean,  colormap, f'{path_figures}E_{season}_{name}_ort_filterafter_annual_clima', units='[m/s]', topography = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "if seasons == True:\n",
    "\n",
    "    lons_20_degrees = int(20/(lons[1]-lons[0]))\n",
    "    \n",
    "    def theta_prim(ncfile_i):\n",
    "        E_prim = np.array(Dataset(f'{path_outputs}temporal/E_thres/{ncfile_i}.nc')['E'])\n",
    "        time_i = np.array(Dataset(f'{path_outputs}temporal/E_thres/{ncfile_i}.nc')['time'])\n",
    "        theta_prim_diff = np.zeros_like(E_prim) * np.nan \n",
    "        theta_prim = np.zeros_like(E_prim) * np.nan \n",
    "\n",
    "        for num, lat in enumerate(lats_NH):\n",
    "            # print('------------------------')\n",
    "            # print(num)\n",
    "            print(lat)\n",
    "            #events = {}\n",
    "            for t in range(1,E_prim.shape[0]-1):\n",
    "            # for t in range(1,10):\n",
    "                serie = E_prim[t,num,:]\n",
    "                if (np.count_nonzero(~np.isnan(serie)) == 0): continue\n",
    "\n",
    "\n",
    "                not_nan_indices = np.diff(np.concatenate(([False], ~np.isnan(serie))))\n",
    "\n",
    "                # Find the indices where the True values start and end\n",
    "                indices = np.where(not_nan_indices == 1)[0]\n",
    "\n",
    "                if len(indices) > 0:\n",
    "                    if (len(serie) > indices[-1]) & (len(indices) % 2 > 0):\n",
    "                        indices = np.append(indices, len(not_nan_indices))\n",
    "\n",
    "                events = np.reshape(indices, [int(len(indices)/2), 2])\n",
    "\n",
    "                for event_i in events:\n",
    "                    # print(event_i)\n",
    "                    if np.count_nonzero(~(serie[event_i[0]:event_i[1]] == np.zeros_like(serie[event_i[0]:event_i[1]]))) == 0: continue \n",
    "                    analytic_signal = hilbert(serie[event_i[0]:event_i[1]] - np.nanmean(serie[event_i[0]:event_i[1]]))\n",
    "                    angles = np.round((np.unwrap((np.angle(analytic_signal))) + 4 * np.pi) % (4 * np.pi) - 2 * np.pi , 3)\n",
    "\n",
    "                    angles[(angles<-3*np.pi/2) & (angles>=-2*np.pi)] = angles[(angles<-3*np.pi/2) & (angles>=-2*np.pi)] + 2*np.pi\n",
    "                    angles[(angles>3*np.pi/2) & (angles<2*np.pi)] = angles[(angles>3*np.pi/2) & (angles<2*np.pi)] + 2*np.pi\n",
    "                    theta_prim[t,num,event_i[0]:event_i[1]] = angles\n",
    "\n",
    "                    len_event = event_i[1]-event_i[0]\n",
    "                                    \n",
    "                    serie_pre_cut = E_prim[t-1, num, max(0,event_i[0]-lons_20_degrees):min(len(serie),event_i[1]+lons_20_degrees)]\n",
    "                    serie_post_cut = E_prim[t+1, num, max(0,event_i[0]-lons_20_degrees):min(len(serie),event_i[1]+lons_20_degrees)]\n",
    "                        \n",
    "                    serie_pre = max_subarray_of_length(serie_pre_cut, len_event) \n",
    "                    serie_post = max_subarray_of_length(serie_post_cut, len_event) \n",
    "\n",
    "                    if (np.count_nonzero(~np.isnan(serie_pre)) == 0) or (np.count_nonzero(~np.isnan(serie_post)) == 0): continue\n",
    "                    else:\n",
    "                        analytic_signal_pre = hilbert(serie_pre[~np.isnan(serie_pre)] - np.nanmean(serie_pre))\n",
    "                        analytic_signal_post = hilbert(serie_post[~np.isnan(serie_post)] - np.nanmean(serie_post))\n",
    "\n",
    "                        phase_pre = np.empty([len_event]) *np.nan\n",
    "                        phase_post = np.empty([len_event]) *np.nan\n",
    "                        angles_pre =  np.round((np.unwrap((np.angle(analytic_signal_pre)))+ 4 * np.pi) % (4 * np.pi) - 2 * np.pi , 3)\n",
    "                        angles_post =  np.round((np.unwrap((np.angle(analytic_signal_post)))+ 4 * np.pi) % (4 * np.pi) - 2 * np.pi , 3)\n",
    "\n",
    "                        angles_pre[(angles_pre<-3*np.pi/2) & (angles_pre>=-2*np.pi)] = angles_pre[(angles_pre<-3*np.pi/2) & (angles_pre>=-2*np.pi)] + 2*np.pi\n",
    "                        angles_post[(angles_post>3*np.pi/2) & (angles_post<2*np.pi)] = angles_post[(angles_post>3*np.pi/2) & (angles_post<2*np.pi)] + 2*np.pi\n",
    "                        phase_pre[int((len_event - len(analytic_signal_pre))/2) : int((len_event - len(analytic_signal_pre))/2) + len(analytic_signal_pre)] = angles_pre\n",
    "                        phase_post[int((len_event - len(analytic_signal_post))/2) : int((len_event - len(analytic_signal_post))/2) + len(analytic_signal_post)] = angles_post\n",
    "\n",
    "                        theta_prim_diff[t, num,event_i[0]:event_i[1]] = phase_post - phase_pre\n",
    "                        \n",
    "\n",
    "        save_nc_3d(f'{path_outputs}temporal/theta_prim_diff/{str(ncfile_i)}.nc', theta_prim_diff, lats_NH, lons, time_i, 'theta_prim')\n",
    "        save_nc_3d(f'{path_outputs}temporal/theta_prim/{str(ncfile_i)}.nc', theta_prim, lats_NH, lons, time_i, 'theta_prim')\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/E_thres/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    for sublist_data in [sublist.tolist() for sublist in np.array_split(list_data, 70)]:\n",
    "        if __name__ == \"__main__\":\n",
    "            with multiprocessing.Pool(processes=160) as pool:\n",
    "                pool.map(theta_prim, sublist_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def w_prim(ncfile_i):\n",
    "        print(ncfile_i)\n",
    "        theta_prim = np.array(Dataset(f'{path_outputs}temporal/theta_prim_diff/{ncfile_i}.nc')['theta_prim'])\n",
    "        time_i = np.array(Dataset(f'{path_outputs}temporal/theta_prim_diff/{ncfile_i}.nc')['time'])\n",
    "        w_prim = np.zeros_like(theta_prim) * np.nan\n",
    "\n",
    "          \n",
    "        for t in range(1,len(time_i[1:-1])):\n",
    "            w_prim_i = pd.DataFrame(theta_prim[t])\n",
    "\n",
    "            w_prim_i[w_prim_i> np.pi] = (-w_prim_i[w_prim_i> np.pi] + 2*np.pi)*1000000 / (2*6*3600) # [rad/s]\n",
    "            w_prim_i[w_prim_i< -np.pi] = (-w_prim_i[w_prim_i< -np.pi] - 2*np.pi)*1000000 / (2*6*3600) # [rad/s]\n",
    "            w_prim_i[(w_prim_i<= np.pi) & (w_prim_i>= -np.pi)] = (-w_prim_i[(w_prim_i<= np.pi) & (w_prim_i>= -np.pi)])*1000000 / (2*6*3600) # [rad/s]\n",
    "\n",
    "            w_prim[t,:,:] = w_prim_i\n",
    "        save_nc_3d(f'{path_outputs}temporal/w_prim/{str(ncfile_i)}.nc', w_prim, lats_NH, lons, time_i, 'w_prim')\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/theta_prim_diff/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    for sublist_data in [sublist.tolist() for sublist in np.array_split(list_data,12)]:\n",
    "        if __name__ == \"__main__\":\n",
    "            with multiprocessing.Pool(processes=90) as pool:\n",
    "                pool.map(w_prim, sublist_data) \n",
    "\n",
    "    \n",
    "\n",
    "    a = 6371000 #[m]\n",
    "    def k_prim(ncfile_i):\n",
    "        print(ncfile_i)\n",
    "        theta_prim = np.array(Dataset(f'{path_outputs}temporal/theta_prim/{ncfile_i}.nc')['theta_prim'])\n",
    "        time_i = np.array(Dataset(f'{path_outputs}temporal/theta_prim/{ncfile_i}.nc')['time'])\n",
    "        k_prim = np.zeros_like(theta_prim) * np.nan\n",
    "        for num_lat in range(len(lats_NH)):\n",
    "            print(lats_NH[num_lat])\n",
    "            k_prim_i = pd.DataFrame(theta_prim[:,num_lat,:] - np.roll(theta_prim[:,num_lat,:], -2, axis=1)) #pre - post\n",
    "\n",
    "            if (np.count_nonzero(~np.isnan(k_prim_i)) == 0): continue\n",
    "            else:\n",
    "                k_prim_i[k_prim_i> np.pi] = (-k_prim_i[k_prim_i> np.pi] + 2*np.pi)*1000000 / (2*abs(lons[1]-lons[0])*1*(np.pi/180) * a*np.cos(np.radians(lats_NH[num_lat])))  # [rad/m]\n",
    "                k_prim_i[k_prim_i< -np.pi] = (-k_prim_i[k_prim_i< -np.pi] - 2*np.pi)*1000000 / (2*abs(lons[1]-lons[0])*1*(np.pi/180) * a*np.cos(np.radians(lats_NH[num_lat]))) # [rad/m]\n",
    "                k_prim_i[(k_prim_i<= np.pi) & (k_prim_i>= -np.pi)] = (-k_prim_i[(k_prim_i<= np.pi) & (k_prim_i>= -np.pi)])*1000000 / (2*abs(lons[1]-lons[0])*1*(np.pi/180) * a*np.cos(np.radians(lats_NH[num_lat]))) # [rad/m]\n",
    "\n",
    "                k_prim[:,num_lat,:] = np.roll(np.array(k_prim_i), 1, axis=1)\n",
    "\n",
    "        k_prim[k_prim==0] = np.nan        \n",
    "        save_nc_3d(f'{path_outputs}temporal/k_prim/{str(ncfile_i)}.nc', k_prim, lats_NH, lons, time_i, 'k_prim')\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/theta_prim/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    for sublist_data in [sublist.tolist() for sublist in np.array_split(list_data, 12)]:\n",
    "        if __name__ == \"__main__\":\n",
    "            with multiprocessing.Pool(processes=90) as pool:\n",
    "                pool.map(k_prim, sublist_data)\n",
    "\n",
    "\n",
    "\n",
    "    def group_vel(ncfile_i): \n",
    "        print(ncfile_i)\n",
    "        w_prim = np.array(Dataset(f'{path_outputs}temporal/w_prim/{ncfile_i}.nc')['w_prim'])\n",
    "        k_prim = np.array(Dataset(f'{path_outputs}temporal/k_prim/{ncfile_i}.nc')['k_prim'])\n",
    "        time_i = np.array(Dataset(f'{path_outputs}temporal/k_prim/{ncfile_i}.nc')['time'])\n",
    "        cg = w_prim / k_prim \n",
    "\n",
    "        save_nc_3d(f'{path_outputs}temporal/Cg/{str(ncfile_i)}.nc', cg, lats_NH, lons, time_i, 'Cg')\n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/w_prim/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "    for i in list_data:\n",
    "        group_vel(i)\n",
    "    \n",
    "\n",
    "    list_data = os.listdir(f'{path_outputs}temporal/Cg/')\n",
    "    list_data = np.sort(np.array([re.findall(r'\\d+', list_data[i])[0] for i in range(len(list_data))]).astype(int))\n",
    "\n",
    "    nco_command = f\"module load nco && {path_ncr} {' '.join([f'{path_outputs}/temporal/Cg/' + str(i) + '.nc' for i in list_data])} {os.path.join(path_outputs, 'temporal', f'Cg_{name}_1979-1999.nc')}\"\n",
    "    result = subprocess.run(nco_command, shell=True)\n",
    "    print(result.returncode)\n",
    "\n",
    "\n",
    "    Cg = np.array(Dataset(f'{path_outputs}temporal/Cg_{name}_1979-1999.nc')['Cg'])\n",
    "    # Cg[abs(Cg) > 100] = np.nan\n",
    "\n",
    "    Cg_seasonal_mean = np.empty([4,len(lats_NH),len(lons)]) * np.nan\n",
    "    time_i = np.array(Dataset(f'{path_outputs}temporal/Cg_{name}_1979-1999.nc')['time'])\n",
    "    dates_d_i = [dt.datetime.strptime(str(int(i)), \"%Y%m%d%H\") for i in time_i]\n",
    "    for num, j, season in zip([0,1,2,3],[[12,1,2], [3,4,5], [6,7,8], [9,10,11]],['DJF','MAM','JJA','SON']):\n",
    "        pos_season = np.where((pd.to_datetime(dates_d_i).month == j[0]) ^ (pd.to_datetime(dates_d_i).month == j[1]) ^ (pd.to_datetime(dates_d_i).month == j[2]))[0]\n",
    "        Cg_seasonal_mean[num,:,:] = np.nanmean(Cg[pos_season,:,:], axis=0)\n",
    "    save_nc_3d(f'{path_outputs}temporal/Cg_seasonal_mean.nc', Cg_seasonal_mean, lats_NH, lons, np.arange(0,4,1).astype(str), 'Cg')\n",
    "\n",
    "\n",
    "    Cg_seasonal_mean = np.array(Dataset(f'{path_outputs}temporal/Cg_seasonal_mean.nc')['Cg'])\n",
    "    for num, season in zip([0,1,2,3],['DJF','MAM','JJA','SON']):\n",
    "        vmin, vmax, colormap = colorm(10, 20, 15, 11, 'RdYlBu_r') \n",
    "        season_mean = Cg_seasonal_mean[num]\n",
    "        \n",
    "        map_orhographic(lons, lats_NH, vmin, vmax, season_mean,  colormap, f'{path_figures}Cg_{season}_{name}_ort_filterafter_annual_clima', units='[m/s]', topography = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "660d6559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    dimensions(sizes): lat(281), lon(1440), time(100)\n",
      "    variables(dimensions): float32 lat(lat), float32 lon(lon), <class 'str'> time(time), float32 theta_prim(time, lat, lon)\n",
      "    groups: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import netCDF4\n",
    "\n",
    "directory_path = \"/depot/wanglei/data/Reanalysis/ERA5/Cg/theta_prim/\"\n",
    "\n",
    "nc_files = [f for f in os.listdir(directory_path) if f.endswith('.nc')]\n",
    "\n",
    "if nc_files:\n",
    "    sample_file_path = os.path.join(directory_path, nc_files[0])\n",
    "    with netCDF4.Dataset(sample_file_path, 'r') as nc_file:\n",
    "        print(nc_file)\n",
    "else:\n",
    "    print(\"no .nc file in this dir.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7d6284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   nan    nan    nan    nan    nan    nan    nan    nan    nan  2.701\n",
      "  2.975  2.881    nan    nan    nan    nan    nan  2.673  2.027  1.123\n",
      "  2.805    nan    nan    nan    nan    nan    nan    nan    nan    nan\n",
      "    nan    nan    nan    nan    nan    nan    nan    nan    nan    nan\n",
      "    nan    nan    nan    nan  2.375  2.33   2.875  2.245  2.209  1.48\n",
      "  2.161  1.856    nan  2.966    nan  2.662  2.697  2.866  2.723  2.461\n",
      "  2.749    nan    nan    nan    nan    nan  2.776  2.007  1.335  0.896\n",
      "  0.48  -0.098 -0.574 -0.805 -0.701  0.601  0.867  1.949    nan    nan\n",
      "    nan    nan    nan    nan    nan    nan    nan  2.76   1.797  0.758\n",
      "  0.165 -2.462 -2.489    nan    nan    nan    nan    nan    nan    nan]\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "\n",
    "# open NetCDF files\n",
    "file_path = \"/depot/wanglei/data/Reanalysis/ERA5/Cg/theta_prim/19790.nc\"\n",
    "dataset = netCDF4.Dataset(file_path, \"r\")\n",
    "\n",
    "# get theta_prim value\n",
    "theta_prim = dataset.variables['theta_prim'][:]\n",
    "\n",
    "# print some theta_prim (e.g., first 100 theta value at (148. 333))\n",
    "print(theta_prim[:1000, 148, 333])\n",
    "\n",
    "# clsoe NetCDF files\n",
    "dataset.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09fc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   nan    nan    nan    nan    nan    nan    nan    nan    nan -2.132\n",
      "  6.953  1.337    nan    nan    nan    nan    nan  4.483  4.404 -2.29\n",
      " -4.128    nan    nan    nan    nan    nan    nan    nan    nan    nan\n",
      "    nan    nan    nan    nan    nan    nan    nan    nan    nan    nan\n",
      "    nan    nan    nan    nan -1.047 -0.546 -0.738 -0.911 -1.164 -1.443\n",
      " -1.251 -1.54     nan -0.932    nan  0.671  0.672  3.776  3.183 -1.926\n",
      " -0.88     nan    nan    nan    nan    nan  5.802 -1.477 -1.932 -2.45\n",
      " -2.804 -3.481  2.572  2.468  2.814  3.975  4.294 -1.609    nan    nan\n",
      "    nan    nan    nan    nan    nan    nan    nan -0.64  -1.943 -2.935\n",
      " -3.711 -4.828 -5.322    nan    nan    nan    nan    nan    nan    nan]\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "\n",
    "# open NetCDF files\n",
    "file_path = \"/depot/wanglei/data/Reanalysis/ERA5/Cg/theta_prim_corrected/19790.nc\"\n",
    "dataset = netCDF4.Dataset(file_path, \"r\")\n",
    "\n",
    "# get theta_prim value\n",
    "theta_prim = dataset.variables['theta_prim'][:]\n",
    "\n",
    "# print some theta_prim (e.g., first 100 theta value at (148. 333))\n",
    "print(theta_prim[:1000, 148, 333])\n",
    "\n",
    "# clsoe NetCDF files\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916064ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "lat not found in /",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3089044/3405712425.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ncfile = Dataset(f'f'/scratch/bell/castanev/{'ERA5'}/f'{'v300'}_{'ERA5'}_6h_1979_1999.nc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mncfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/scratch/negishi/yan481/KenYan_Traffic_Model/Interim_Slice_6hr/1979_1979_01_01_06_00_u.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpos_NH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlats\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlats_NH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlats\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: lat not found in /"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "# ncfile = Dataset(f'f'/scratch/bell/castanev/{'ERA5'}/f'{'v300'}_{'ERA5'}_6h_1979_1999.nc')\n",
    "ncfile = Dataset(f'/scratch/negishi/yan481/KenYan_Traffic_Model/Interim_Slice_6hr/1979_1979_01_01_06_00_u.nc')\n",
    "lats = np.array(ncfile['lat'])\n",
    "pos_NH = np.where(lats>=20)[0]\n",
    "lats_NH = lats[lats>=20]\n",
    "lons = np.array(ncfile['lon'])\n",
    "time = np.array(ncfile['time'])[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ece752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nc_3d(path_name, var, lats, lons, dates_str, var_str):\n",
    "    ncfile = Dataset(path_name, 'w')\n",
    "    ncfile.createDimension('lat', len(lats))\n",
    "    ncfile.createDimension('lon', len(lons))\n",
    "    ncfile.createDimension('time', None)\n",
    "    var_lats = ncfile.createVariable('lat', 'f', ('lat'))\n",
    "    var_lons = ncfile.createVariable('lon', 'f', ('lon'))\n",
    "    var_time = ncfile.createVariable('time', 'str', ('time'))\n",
    "    var_lats[:] = lats\n",
    "    var_lons[:] = lons\n",
    "    var_time[:] = dates_str\n",
    "    varr = ncfile.createVariable(var_str, 'f', ('time', 'lat', 'lon'), fill_value=None)\n",
    "    varr[:, :, :] = var\n",
    "    ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600696bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19865.nc19851.nc19804.nc19840.nc19809.nc19813.nc19814.nc19792.nc19815.nc19839.nc19843.nc19812.nc19799.nc19837.nc\n",
      "\n",
      "19794.nc19841.nc19862.nc19877.nc\n",
      "19824.nc\n",
      "\n",
      "19826.nc19850.nc\n",
      "\n",
      "19800.nc19873.nc19834.nc\n",
      "19795.nc19831.nc\n",
      "19844.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19805.nc19790.nc\n",
      "19866.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19846.nc\n",
      "19855.nc\n",
      "19832.nc\n",
      "19860.nc\n",
      "19854.nc\n",
      "19797.nc\n",
      "19821.nc\n",
      "19829.nc\n",
      "19847.nc\n",
      "19864.nc\n",
      "19859.nc\n",
      "19878.nc\n",
      "19823.nc\n",
      "19845.nc\n",
      "19853.nc\n",
      "19874.nc\n",
      "19791.nc\n",
      "19811.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88 [03:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lats_NH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/apps/spack/negishi/apps/anaconda/2022.10-py39-gcc-8.5.0-sjvibry/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/tmp/ipykernel_3089044/104657282.py\", line 35, in w_prim\n    save_nc_3d(f'/scratch/negishi/yan481/KenYan_Traffic_Model/Valentina Cg Calculation/{str(ncfile_i)}.nc', w_prim, lats_NH, lons, time_i, 'w_prim')\nNameError: name 'lats_NH' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3089044/104657282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_prim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/apps/spack/negishi/apps/anaconda/2022.10-py39-gcc-8.5.0-sjvibry/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/spack/negishi/apps/anaconda/2022.10-py39-gcc-8.5.0-sjvibry/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lats_NH' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_wrapped_derivative(prev_val, curr_val, next_val, delta):\n",
    "    diff = next_val - prev_val\n",
    "\n",
    "    if diff > np.pi:\n",
    "        return (prev_val + 2 * np.pi - next_val) / (2 * delta)\n",
    "    elif diff < -np.pi:\n",
    "        return (prev_val - 2 * np.pi - next_val) / (2 * delta)\n",
    "    else:\n",
    "        return (prev_val - next_val) / (2 * delta)\n",
    "\n",
    "\n",
    "def w_prim(ncfile_i):\n",
    "    print(ncfile_i)\n",
    "    theta_prim = np.array(Dataset(f'/depot/wanglei/data/Reanalysis/ERA5/Cg/theta_prim/{ncfile_i}')['theta_prim'])\n",
    "    time_i = np.array(Dataset(f'/depot/wanglei/data/Reanalysis/ERA5/Cg/theta_prim/{ncfile_i}')['time'])\n",
    "    w_prim = np.zeros_like(theta_prim) * np.nan\n",
    "\n",
    "    delta_t = 6 * 3600  # assuming delta_t is in seconds\n",
    "\n",
    "    for t in range(1, len(time_i[1:-1])):\n",
    "        for j in range(theta_prim.shape[1]):\n",
    "            for k in range(theta_prim.shape[2]):\n",
    "                w_prim[t, j, k] = -calculate_wrapped_derivative(\n",
    "                    theta_prim[t-1, j, k], \n",
    "                    theta_prim[t, j, k], \n",
    "                    theta_prim[t+1, j, k], \n",
    "                    delta_t\n",
    "                )\n",
    "    save_nc_3d(f'/scratch/negishi/yan481/KenYan_Traffic_Model/Valentina Cg Calculation/{str(ncfile_i)}.nc', w_prim, lats_NH, lons, time_i, 'w_prim')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path_to_data = \"/depot/wanglei/data/Reanalysis/ERA5/Cg/theta_prim/\"\n",
    "    list_data = [f for f in os.listdir(path_to_data) if f.endswith('.nc')]\n",
    "\n",
    "    num_processes = 32\n",
    "\n",
    "    with Pool(num_processes) as pool:\n",
    "        list(tqdm(pool.imap(w_prim, list_data), total=len(list_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "import re\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "path_inputs = '/depot/wanglei/data/Reanalysis/ERA5/Cg/theta_prim/'\n",
    "path_outputs = '/scratch/negishi/yan481/KenYan_Traffic_Model/Valentina Cg Calculation/'\n",
    "\n",
    "a = 6371000  # Earth radius in meters\n",
    "\n",
    "def calculate_wrapped_derivative(prev_val, curr_val, next_val, delta):\n",
    "    diff = next_val - prev_val\n",
    "\n",
    "    if diff > np.pi:\n",
    "        return (prev_val + 2 * np.pi - next_val) / (2 * delta)\n",
    "    elif diff < -np.pi:\n",
    "        return (prev_val - 2 * np.pi - next_val) / (2 * delta)\n",
    "    else:\n",
    "        return (prev_val - next_val) / (2 * delta)\n",
    "\n",
    "def k_prim(ncfile_i):\n",
    "    print(ncfile_i)\n",
    "    theta_prim = np.array(Dataset(f'{path_inputs}{ncfile_i}.nc')['theta_prim'])\n",
    "    time_i = np.array(Dataset(f'{path_inputs}{ncfile_i}.nc')['time'])\n",
    "    k_prim = np.zeros_like(theta_prim) * np.nan\n",
    "\n",
    "    delta_x = 2 * np.pi / 180  # 2 degrees in radians\n",
    "\n",
    "    for num_lat in range(len(lats_NH)):\n",
    "        for j in range(1, theta_prim.shape[1]-1):\n",
    "            for k in range(1, theta_prim.shape[2]-1):\n",
    "                k_prim[num_lat, j, k] = -calculate_wrapped_derivative(\n",
    "                    theta_prim[num_lat, j-1, k], \n",
    "                    theta_prim[num_lat, j, k], \n",
    "                    theta_prim[num_lat, j+1, k], \n",
    "                    delta_x\n",
    "                ) / (a * np.cos(np.radians(lats_NH[num_lat])))\n",
    "\n",
    "    k_prim[k_prim==0] = np.nan\n",
    "    save_nc_3d(f'{path_outputs}temporal/k_prim/{str(ncfile_i)}.nc', k_prim, lats_NH, lons, time_i, 'k_prim')\n",
    "\n",
    "# I'll make a slight adjustment for fetching the list of data\n",
    "list_data = [f.split('.nc')[0] for f in os.listdir(path_inputs) if f.endswith('.nc')]\n",
    "list_data = np.sort(np.array([re.findall(r'\\d+', file)[0] for file in list_data]).astype(int))\n",
    "\n",
    "for sublist_data in [sublist.tolist() for sublist in np.array_split(list_data, 12)]:\n",
    "    if __name__ == \"__main__\":\n",
    "        with multiprocessing.Pool(processes=32) as pool:\n",
    "            pool.map(k_prim, sublist_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Anaconda 2022.10)",
   "language": "python",
   "name": "anaconda-2022.10-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
